
@inproceedings{tishby_information_2017,
	location = {Berlin},
	title = {Information Theory of Deep Learning},
	url = {https://www.youtube.com/watch?v=bLqJHjXihK8},
	eventtitle = {Deep Learning: Theory, Algorithms, and Applications},
	author = {Tishby, Naftali},
	urldate = {2018-04-05},
	date = {2017-06-25},
	keywords = {Combinatorial Pattern Matching, {CPM}, Yandex, Яндекс}
}

@article{cauchy_methode_1847,
	title = {Méthode générale pour la résolution des systemes d’équations simultanées},
	volume = {25},
	url = {https://www.cs.xu.edu/math/Sources/Cauchy/Orbits/1847%20CR%20536(383).pdf},
	pages = {536--538},
	number = {1847},
	journaltitle = {Comp. Rend. Sci. Paris},
	author = {Cauchy, Augustin},
	urldate = {2017-07-04},
	date = {1847}
}

@book{goodfellow_deep_2016,
	location = {Cambridge, Massachusetts},
	title = {Deep Learning},
	isbn = {978-0-262-03561-3},
	abstract = {"Written by three experts in the field,  Deep Learning is the only comprehensive book on the subject." -- Elon Musk, cochair of {OpenAI}; cofounder and {CEO} of Tesla and {SpaceXDeep} learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models.  Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	pagetotal = {800},
	publisher = {The {MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016-11-18}
}

@book{knuth_art_1968,
	title = {The Art of Computer Programming},
	isbn = {0-201-03801-3},
	publisher = {Addison-Wesley},
	author = {Knuth, Donald E.},
	date = {1968},
	langid = {english}
}

@article{amores_multiple_2013,
	title = {Multiple instance classification: Review, taxonomy and comparative study},
	volume = {201},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370213000581},
	doi = {10.1016/j.artint.2013.06.003},
	shorttitle = {Multiple instance classification},
	abstract = {Multiple Instance Learning ({MIL}) has become an important topic in the pattern recognition community, and many solutions to this problem have been proposed until now. Despite this fact, there is a lack of comparative studies that shed light into the characteristics and behavior of the different methods. In this work we provide such an analysis focused on the classification task (i.e., leaving out other learning tasks such as regression). In order to perform our study, we implemented fourteen methods grouped into three different families. We analyze the performance of the approaches across a variety of well-known databases, and we also study their behavior in synthetic scenarios in order to highlight their characteristics. As a result of this analysis, we conclude that methods that extract global bag-level information show a clearly superior performance in general. In this sense, the analysis permits us to understand why some types of methods are more successful than others, and it permits us to establish guidelines in the design of new {MIL} methods.},
	pages = {81--105},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Amores, Jaume},
	urldate = {2017-05-31},
	date = {2013-08},
	keywords = {Bag-of-Words, Codebook, Multi-instance learning}
}

@article{dietterich_solving_1997,
	title = {Solving the multiple instance problem with axis-parallel rectangles},
	volume = {89},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370296000343},
	doi = {10.1016/S0004-3702(96)00034-3},
	abstract = {The multiple instance problem arises in tasks where the training examples are ambiguous: a single example object may have many alternative feature vectors (instances) that describe it, and yet only one of those feature vectors may be responsible for the observed classification of the object. This paper describes and compares three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem. Algorithms that ignore the multiple instance problem perform very poorly. An algorithm that directly confronts the multiple instance problem (by attempting to identify which feature vectors are responsible for the observed classifications) performs best, giving 89\% correct predictions on a musk odor prediction task. The paper also illustrates the use of artificial data to debug and compare these algorithms.},
	pages = {31--71},
	number = {1},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Dietterich, Thomas G. and Lathrop, Richard H. and Lozano-Pérez, Tomás},
	urldate = {2017-05-31},
	date = {1997-01-01},
	keywords = {Drug design, Machine learning, Structure-activity relationships}
}

@article{kingma_adam:_2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2017-04-19},
	date = {2014-12-22},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Learning}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	rights = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	urldate = {2018-04-05},
	date = {1986-10},
	langid = {english}
}

@incollection{nokland_direct_2016,
	title = {Direct Feedback Alignment Provides Learning in Deep Neural Networks},
	url = {http://papers.nips.cc/paper/6441-direct-feedback-alignment-provides-learning-in-deep-neural-networks.pdf},
	pages = {1037--1045},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Nøkland, Arild},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	date = {2016}
}

@article{lillicrap_random_2016,
	title = {Random synaptic feedback weights support error backpropagation for deep learning},
	volume = {7},
	url = {http://dx.doi.org/10.1038/ncomms13276},
	pages = {13276},
	journaltitle = {Nature Communications},
	shortjournal = {Nature Communications},
	author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
	date = {2016-11-08}
}

@incollection{lecun_learning_1986,
	title = {Learning Process in an Asymmetric Threshold Network},
	isbn = {978-3-642-82657-3},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-82657-3_24},
	series = {{NATO} {ASI} Series},
	abstract = {Threshold functions and related operators are widely used as basic elements of adaptive and associative networks [Nakano 72, Amari 72, Hopfield 82]. There exist numerous learning rules for finding a set of weights to achieve a particular correspondence between input-output pairs. But early works in the field have shown that the number of threshold functions (or linearly separable functions) in N binary variables is small compared to the number of all possible boolean mappings in N variables, especially if N is large. This problem is one of the main limitations of most neural networks models where the state is fully specified by the environment during learning: they can only learn linearly separable functions of their inputs. Moreover, a learning procedure which requires the outside world to specify the state of every neuron during the learning session can hardly be considered as a general learning rule because in real-world conditions, only a partial information on the “ideal” network state for each task is available from the environment. It is possible to use a set of so-called “hidden units” [Hinton,Sejnowski,Ackley. 84], without direct interaction with the environment, which can compute intermediate predicates. Unfortunately, the global response depends on the output of a particular hidden unit in a highly non-linear way, moreover the nature of this dependence is influenced by the states of the other cells.},
	pages = {233--240},
	booktitle = {Disordered Systems and Biological Organization},
	publisher = {Springer, Berlin, Heidelberg},
	author = {{LeCun}, Yann},
	urldate = {2018-04-11},
	date = {1986},
	langid = {english},
	doi = {10.1007/978-3-642-82657-3_24}
}

@article{bengio_how_2014,
	title = {How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation},
	url = {http://arxiv.org/abs/1407.7906},
	abstract = {We propose to exploit \{{\textbackslash}em reconstruction\} as a layer-local training signal for deep learning. Reconstructions can be propagated in a form of target propagation playing a role similar to back-propagation but helping to reduce the reliance on derivatives in order to perform credit assignment across many levels of possibly strong non-linearities (which is difficult for back-propagation). A regularized auto-encoder tends produce a reconstruction that is a more likely version of its input, i.e., a small move in the direction of higher likelihood. By generalizing gradients, target propagation may also allow to train deep networks with discrete hidden units. If the auto-encoder takes both a representation of input and target (or of any side information) in input, then its reconstruction of input representation provides a target towards a representation that is more likely, conditioned on all the side information. A deep auto-encoder decoding path generalizes gradient propagation in a learned way that can could thus handle not just infinitesimal changes but larger, discrete changes, hopefully allowing credit assignment through a long chain of non-linear operations. In addition to each layer being a good auto-encoder, the encoder also learns to please the upper layers by transforming the data into a space where it is easier to model by them, flattening manifolds and disentangling factors. The motivations and theoretical justifications for this approach are laid down in this paper, along with conjectures that will have to be verified either mathematically or experimentally, including a hypothesis stating that such auto-encoder mediated target propagation could play in brains the role of credit assignment through many non-linear, noisy and discrete transformations.},
	journaltitle = {{arXiv}:1407.7906 [cs]},
	author = {Bengio, Yoshua},
	urldate = {2018-04-11},
	date = {2014-07-29},
	eprinttype = {arxiv},
	eprint = {1407.7906},
	keywords = {Computer Science - Learning}
}

@article{bengio_towards_2015,
	title = {Towards Biologically Plausible Deep Learning},
	url = {http://arxiv.org/abs/1502.04156},
	abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational {EM} algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
	journaltitle = {{arXiv}:1502.04156 [cs]},
	author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
	urldate = {2018-04-11},
	date = {2015-02-13},
	eprinttype = {arxiv},
	eprint = {1502.04156},
	keywords = {Computer Science - Learning}
}

@online{olah_calculus_2015,
	title = {Calculus on Computational Graphs: Backpropagation},
	url = {https://colah.github.io/posts/2015-08-Backprop/},
	titleaddon = {colah's blog},
	author = {Olah, Christopher},
	urldate = {2018-04-22},
	date = {2015-08-31}
}

@inproceedings{lee_difference_2015,
	title = {Difference Target Propagation},
	isbn = {978-3-319-23528-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-23528-8_31},
	doi = {10.1007/978-3-319-23528-8_31},
	series = {Lecture Notes in Computer Science},
	abstract = {Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of non-linearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks.},
	eventtitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	pages = {498--515},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	publisher = {Springer, Cham},
	author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
	urldate = {2018-05-02},
	date = {2015-09-07},
	langid = {english}
}

@article{bourlard_auto-association_1988,
	title = {Auto-association by multilayer perceptrons and singular value decomposition},
	volume = {59},
	issn = {0340-1200, 1432-0770},
	url = {https://link.springer.com/article/10.1007/BF00332918},
	doi = {10.1007/BF00332918},
	abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Loève transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the rôle of the different parameters.},
	pages = {291--294},
	number = {4},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybern.},
	author = {Bourlard, H. and Kamp, Y.},
	urldate = {2018-05-03},
	date = {1988-09-01},
	langid = {english}
}

@thesis{dedic_hierarchicke_2017,
	location = {Prague},
	title = {Hierarchické modely síťového provozu},
	abstract = {The current approach to the detection of unwanted software by monitoring client traffic uses
handwritten features as part of the model. This approach has several disadvantages. This thesis proposes
a fully automated classifier which recognises malware activity at network connection level. The multi-
instance learning approach was used in order to achieve this. As a part of this thesis the multi-instance
learning was theoretically defined using two different formalisms and current work in this field was sum-
marised. Subsequently, there was described the hierarchical structure of an {URL} address which was used
as an input for the classifier. A model reflecting this inherent hierarchical structure was proposed and
an explanation of how multi-instance learning was utilised and modified was presented together with the
description of the implementation of the model using neural networks. Methods of classifier quality as-
sessment were outlined. The classifier presented here was compared with prior art and the influence of
model parameters on its quality was assessed.},
	pagetotal = {41},
	institution = {Czech Technical University in Prague},
	type = {Bachelor thesis},
	author = {Dědič, Marek},
	date = {2017-07-07}
}

@article{zhang_multi-instance_2009,
	title = {Multi-instance clustering with applications to multi-instance prediction},
	volume = {31},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/article/10.1007/s10489-007-0111-x},
	doi = {10.1007/s10489-007-0111-x},
	abstract = {In the setting of multi-instance learning, each object is represented by a bag composed of multiple instances instead of by a single instance in a traditional learning setting. Previous works in this area only concern multi-instance prediction problems where each bag is associated with a binary (classification) or real-valued (regression) label. However, unsupervised multi-instance learning where bags are without labels has not been studied. In this paper, the problem of unsupervised multi-instance learning is addressed where a multi-instance clustering algorithm named Bamic is proposed. Briefly, by regarding bags as atomic data items and using some form of distance metric to measure distances between bags, Bamic adapts the popular k -Medoids algorithm to partition the unlabeled training bags into k disjoint groups of bags. Furthermore, based on the clustering results, a novel multi-instance prediction algorithm named Bartmip is developed. Firstly, each bag is re-represented by a k-dimensional feature vector, where the value of the i-th feature is set to be the distance between the bag and the medoid of the i-th group. After that, bags are transformed into feature vectors so that common supervised learners are used to learn from the transformed feature vectors each associated with the original bag’s label. Extensive experiments show that Bamic could effectively discover the underlying structure of the data set and Bartmip works quite well on various kinds of multi-instance prediction problems.},
	pages = {47--68},
	number = {1},
	journaltitle = {Applied Intelligence},
	shortjournal = {Appl Intell},
	author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
	urldate = {2018-05-14},
	date = {2009-08-01},
	langid = {english}
}

@inproceedings{chen_contextual_2012,
	location = {Sichuan, China},
	title = {Contextual Hausdorff dissimilarity for multi-instance clustering},
	isbn = {978-1-4673-0024-7},
	url = {https://ieeexplore.ieee.org/abstract/document/6233889/},
	doi = {10.1109/FSKD.2012.6233889},
	eventtitle = {2012 9th International Conference on Fuzzy Systems and Knowledge Discovery},
	pages = {870--873},
	booktitle = {Fuzzy Systems and Knowledge Discovery ({FSKD})},
	publisher = {{IEEE} Computer Society Press},
	author = {Chen, Ying and Wu, Ou},
	urldate = {2018-05-14},
	date = {2012-05}
}