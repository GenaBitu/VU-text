
@article{chen_miles:_2006,
	title = {{MILES}: Multiple-Instance Learning via Embedded Instance Selection},
	volume = {28},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2006.248},
	shorttitle = {{MILES}},
	abstract = {Multiple-instance problems arise from the situations where training class labels are attached to sets of samples (named bags), instead of individual samples within each bag (called instances). Most previous multiple-instance learning ({MIL}) algorithms are developed based on the assumption that a bag is positive if and only if at least one of its instances is positive. Although the assumption works well in a drug activity prediction problem, it is rather restrictive for other applications, especially those in the computer vision area. We propose a learning method, {MILES} (multiple-instance learning via embedded instance selection), which converts the multiple-instance learning problem to a standard supervised learning problem that does not impose the assumption relating instance labels to bag labels. {MILES} maps each bag into a feature space defined by the instances in the training bags via an instance similarity measure. This feature mapping often provides a large number of redundant or irrelevant features. Hence, 1-norm {SVM} is applied to select important features as well as construct classifiers simultaneously. We have performed extensive experiments. In comparison with other methods, {MILES} demonstrates competitive classification accuracy, high computation efficiency, and robustness to labeling uncertainty},
	pages = {1931--1947},
	number = {12},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, Yixin and Bi, Jinbo and Wang, J. Z.},
	date = {2006-12},
	keywords = {1-norm support vector machine, Algorithms, Application software, Artificial Intelligence, classification accuracy, computer vision, drug activity prediction, drug activity prediction., Drugs, embedded instance selection, feature extraction, feature mapping, feature subset selection, image categorization, Image Enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Information Storage and Retrieval, Labeling, labeling uncertainty, learning (artificial intelligence), Learning systems, {MILES}, Multiple-instance learning, multiple-instance learning algorithms, object recognition, pattern classification, Reproducibility of Results, Robustness, Sensitivity and Specificity, supervised learning, support vector machine, Support vector machine classification, support vector machines, Uncertainty}
}

@thesis{foulds_learning_2008,
	title = {Learning Instance Weights in Multi-Instance Learning},
	url = {http://researchcommons.waikato.ac.nz/handle/10289/2460},
	abstract = {Multi-instance ({MI}) learning is a variant of supervised machine learning, where each learning example contains a bag of instances instead of just a single feature vector. {MI} learning has applications in areas such as drug activity prediction, fruit disease management and image classification.

This thesis investigates the case where each instance has a weight value determining the level of influence that it has on its bag's class label. This is a more general assumption than most existing approaches use, and thus is more widely applicable. The challenge is to accurately estimate these weights in order to make predictions at the bag level.

An existing approach known as {MILES} is retroactively identified as an algorithm that uses instance weights for {MI} learning, and is evaluated using a variety of base learners on benchmark problems. New algorithms for learning instance weights for {MI} learning are also proposed and rigorously evaluated on both artificial and real-world datasets. The new algorithms are shown to achieve better root mean squared error rates than existing approaches on artificial data generated according to the algorithms' underlying assumptions. Experimental results also demonstrate that the new algorithms are competitive with existing approaches on real-world problems.},
	institution = {The University of Waikato},
	type = {Thesis},
	author = {Foulds, James Richard},
	urldate = {2017-07-06},
	date = {2008},
	langid = {english}
}

@article{dempster_maximum_1977,
	title = {Maximum Likelihood from Incomplete Data via the {EM} Algorithm},
	volume = {39},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2984875},
	abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	pages = {1--38},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	urldate = {2017-07-01},
	date = {1977}
}

@inproceedings{wang_solving_2000,
	location = {Stanford University, Stanford, {CA}, {USA}},
	title = {Solving Multiple-Instance Problem: A Lazy Learning Approach},
	url = {http://cogprints.org/2124/},
	shorttitle = {Solving Multiple-Instance Problem},
	abstract = {As opposed to traditional supervised learning, multiple-instance learning 
    concerns the problem of classifying a bag of instances, given bags that are 
    labeled by a teacher as being overall positive or negative. Current research 
    mainly concentrates on adapting traditional concept learning to solve this 
    problem. In this paper we investigate the use of lazy learning and Hausdorff 
    distance to approach the multiple-instance problem. We present two variants of 
    the K-nearest neighbor algorithm, called Bayesian-{KNN} and Citation-{KNN}, solving 
    the multiple-instance problem. Experiments on the Drug discovery benchmark data 
    show that both algorithms are competitive with the best ones conceived in the 
    concept learning framework. Further work includes exploring of a combination of 
    lazy and eager multiple-instance problem classifiers.},
	eventtitle = {Seventeenth International Conference on Machine Learning},
	pages = {1119--1125},
	booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
	publisher = {Morgan Kaufmann},
	author = {Wang, Jun and Zucker, Jean-Daniel},
	editor = {Langley, Pat},
	urldate = {2017-07-01},
	date = {2000}
}

@inproceedings{zhou_multi-instance_2009,
	location = {New York, {NY}, {USA}},
	title = {Multi-instance Learning by Treating Instances As non-I.I.D. Samples},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553534},
	doi = {10.1145/1553374.1553534},
	series = {{ICML} '09},
	abstract = {Previous studies on multi-instance learning typically treated instances in the bags as independently and identically distributed. The instances in a bag, however, are rarely independent in real tasks, and a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits relations among instances. In this paper, we propose two simple yet effective methods. In the first method, we explicitly map every bag to an undirected graph and design a graph kernel for distinguishing the positive and negative bags. In the second method, we implicitly construct graphs by deriving affinity matrices and propose an efficient graph kernel considering the clique information. The effectiveness of the proposed methods are validated by experiments.},
	pages = {1249--1256},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Zhou, Zhi-Hua and Sun, Yu-Yin and Li, Yu-Feng},
	urldate = {2017-07-06},
	date = {2009}
}

@article{chen_image_2004,
	title = {Image Categorization by Learning and Reasoning with Regions},
	volume = {5},
	issn = {{ISSN} 1533-7928},
	url = {http://www.jmlr.org/papers/v5/chen04a.html},
	pages = {913--939},
	issue = {Aug},
	journaltitle = {Journal of Machine Learning Research},
	author = {Chen, Yixin and Wang, James Z.},
	urldate = {2017-07-06},
	date = {2004}
}

@article{cauchy_methode_1847,
	title = {Méthode générale pour la résolution des systemes d’équations simultanées},
	volume = {25},
	url = {https://www.cs.xu.edu/math/Sources/Cauchy/Orbits/1847%20CR%20536(383).pdf},
	pages = {536--538},
	number = {1847},
	journaltitle = {Comp. Rend. Sci. Paris},
	author = {Cauchy, Augustin},
	urldate = {2017-07-04},
	date = {1847}
}

@article{cheplygina_multiple_2015,
	title = {Multiple instance learning with bag dissimilarities},
	volume = {48},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320314002817},
	doi = {10.1016/j.patcog.2014.07.022},
	abstract = {Multiple instance learning ({MIL}) is concerned with learning from sets (bags) of objects (instances), where the individual instance labels are ambiguous. In this setting, supervised learning cannot be applied directly. Often, specialized {MIL} methods learn by making additional assumptions about the relationship of the bag labels and instance labels. Such assumptions may fit a particular dataset, but do not generalize to the whole range of {MIL} problems. Other {MIL} methods shift the focus of assumptions from the labels to the overall (dis)similarity of bags, and therefore learn from bags directly. We propose to represent each bag by a vector of its dissimilarities to other bags in the training set, and treat these dissimilarities as a feature representation. We show several alternatives to define a dissimilarity between bags and discuss which definitions are more suitable for particular {MIL} problems. The experimental results show that the proposed approach is computationally inexpensive, yet very competitive with state-of-the-art algorithms on a wide range of {MIL} datasets.},
	pages = {264--275},
	number = {1},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Cheplygina, Veronika and Tax, David M. J. and Loog, Marco},
	urldate = {2017-07-04},
	date = {2015-01-01},
	keywords = {drug activity prediction, Dissimilarity representation, Image classification, Multiple instance learning, Point set distance, Text categorization}
}

@report{haussler_convolution_1999,
	title = {Convolution kernels on discrete structures},
	url = {https://www.soe.ucsc.edu/sites/default/files/technical-reports/UCSC-CRL-99-10.pdf},
	institution = {Technical report, Department of Computer Science, University of California at Santa Cruz},
	author = {Haussler, David},
	urldate = {2017-07-04},
	date = {1999}
}

@inproceedings{kwok_marginalized_2007,
	title = {Marginalized Multi-Instance Kernels.},
	volume = {7},
	url = {http://www.aaai.org/Papers/IJCAI/2007/IJCAI07-145.pdf},
	pages = {901--906},
	booktitle = {{IJCAI}},
	author = {Kwok, James T. and Cheung, Pak-Ming},
	urldate = {2017-07-04},
	date = {2007}
}

@inproceedings{gartner_multi-instance_2002,
	title = {Multi-instance kernels},
	volume = {2},
	url = {http://sci2s.ugr.es/keel/pdf/algorithm/congreso/2002-Gartner-ICML.pdf},
	pages = {179--186},
	booktitle = {{ICML}},
	author = {Gärtner, Thomas and Flach, Peter A. and Kowalczyk, Adam and Smola, Alexander J.},
	urldate = {2017-07-04},
	date = {2002}
}

@inproceedings{zhang_multiple_2006,
	title = {Multiple instance boosting for object detection},
	url = {http://papers.nips.cc/paper/2926-multiple-instance-boosting-for-object-detection.pdf},
	pages = {1417--1424},
	booktitle = {Advances in neural information processing systems},
	author = {Zhang, Cha and Platt, John C. and Viola, Paul A.},
	urldate = {2017-07-04},
	date = {2006}
}

@inproceedings{andrews_support_2003,
	title = {Support vector machines for multiple-instance learning},
	url = {http://papers.nips.cc/paper/2232-support-vector-machines-for-multiple-instance-learning.pdf},
	pages = {577--584},
	booktitle = {Advances in neural information processing systems},
	author = {Andrews, Stuart and Tsochantaridis, Ioannis and Hofmann, Thomas},
	urldate = {2017-07-04},
	date = {2003}
}

@book{goodfellow_deep_2016,
	location = {Cambridge, Massachusetts},
	title = {Deep Learning},
	isbn = {978-0-262-03561-3},
	abstract = {"Written by three experts in the field,  Deep Learning is the only comprehensive book on the subject." -- Elon Musk, cochair of {OpenAI}; cofounder and {CEO} of Tesla and {SpaceXDeep} learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models.  Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	pagetotal = {800},
	publisher = {The {MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016-11-18}
}

@inproceedings{zhu_1-norm_2004,
	title = {1-norm support vector machines},
	url = {http://papers.nips.cc/paper/2450-1-norm-support-vector-machines.pdf},
	pages = {49--56},
	booktitle = {Advances in neural information processing systems},
	author = {Zhu, Ji and Rosset, Saharon and Tibshirani, Robert and Hastie, Trevor J.},
	urldate = {2017-07-01},
	date = {2004}
}

@book{dasarathy_nearest_1991,
	title = {Nearest neighbor ({NN}) norms: nn pattern classification techniques},
	isbn = {978-0-8186-5930-0},
	shorttitle = {Nearest neighbor ({NN}) norms},
	pagetotal = {474},
	publisher = {{IEEE} Computer Society Press},
	author = {Dasarathy, Belur V.},
	date = {1991},
	langid = {english},
	note = {Google-Books-{ID}: k2dQAAAAMAAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Psychology / Cognitive Psychology}
}

@inproceedings{maron_framework_1998,
	title = {A framework for multiple-instance learning},
	url = {http://papers.nips.cc/paper/1346-a-framework-for-multiple-instance-learning.pdf},
	pages = {570--576},
	booktitle = {Advances in neural information processing systems},
	author = {Maron, Oded and Lozano-Pérez, Tomás},
	urldate = {2017-07-01},
	date = {1998}
}

@inproceedings{zhang_em-dd:_2002,
	title = {{EM}-{DD}: An Improved Multiple-Instance Learning Technique},
	url = {http://papers.nips.cc/paper/1959-em-dd-an-improved-multiple-instance-learning-technique.pdf},
	shorttitle = {{EM}-{DD}},
	pages = {1073--1080},
	booktitle = {Advances in neural information processing systems},
	author = {Zhang, Qi and Goldman, Sally A.},
	urldate = {2017-07-01},
	date = {2002}
}

@inproceedings{muandet_learning_2012,
	title = {Learning from Distributions via Support Measure Machines},
	url = {http://papers.nips.cc/paper/4825-learning-from-distributions-via-support-measure-machines},
	pages = {10--18},
	booktitle = {Advances in neural information processing systems},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Dinuzzo, Francesco and Schölkopf, Bernhard},
	urldate = {2017-06-29},
	date = {2012}
}

@book{knuth_art_1968,
	title = {The Art of Computer Programming},
	isbn = {0-201-03801-3},
	publisher = {Addison-Wesley},
	author = {Knuth, Donald E.},
	date = {1968},
	langid = {english}
}

@article{amores_multiple_2013,
	title = {Multiple instance classification: Review, taxonomy and comparative study},
	volume = {201},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370213000581},
	doi = {10.1016/j.artint.2013.06.003},
	shorttitle = {Multiple instance classification},
	abstract = {Multiple Instance Learning ({MIL}) has become an important topic in the pattern recognition community, and many solutions to this problem have been proposed until now. Despite this fact, there is a lack of comparative studies that shed light into the characteristics and behavior of the different methods. In this work we provide such an analysis focused on the classification task (i.e., leaving out other learning tasks such as regression). In order to perform our study, we implemented fourteen methods grouped into three different families. We analyze the performance of the approaches across a variety of well-known databases, and we also study their behavior in synthetic scenarios in order to highlight their characteristics. As a result of this analysis, we conclude that methods that extract global bag-level information show a clearly superior performance in general. In this sense, the analysis permits us to understand why some types of methods are more successful than others, and it permits us to establish guidelines in the design of new {MIL} methods.},
	pages = {81--105},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Amores, Jaume},
	urldate = {2017-05-31},
	date = {2013-08},
	keywords = {Bag-of-Words, Codebook, Multi-instance learning}
}

@inproceedings{zhou_neural_2002,
	title = {Neural Networks for Multi-Instance Learning},
	url = {http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/techrep02.pdf},
	pages = {455--459},
	booktitle = {Proceedings of the International Conference on Intelligent Information Technology, Beijing, China},
	author = {Zhou, Zhi-Hua and Zhang, Min-Ling},
	urldate = {2017-06-26},
	date = {2002-08},
	langid = {english}
}

@article{dietterich_solving_1997,
	title = {Solving the multiple instance problem with axis-parallel rectangles},
	volume = {89},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370296000343},
	doi = {10.1016/S0004-3702(96)00034-3},
	abstract = {The multiple instance problem arises in tasks where the training examples are ambiguous: a single example object may have many alternative feature vectors (instances) that describe it, and yet only one of those feature vectors may be responsible for the observed classification of the object. This paper describes and compares three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem. Algorithms that ignore the multiple instance problem perform very poorly. An algorithm that directly confronts the multiple instance problem (by attempting to identify which feature vectors are responsible for the observed classifications) performs best, giving 89\% correct predictions on a musk odor prediction task. The paper also illustrates the use of artificial data to debug and compare these algorithms.},
	pages = {31--71},
	number = {1},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Dietterich, Thomas G. and Lathrop, Richard H. and Lozano-Pérez, Tomás},
	urldate = {2017-05-31},
	date = {1997-01-01},
	keywords = {Drug design, Machine learning, Structure-activity relationships}
}

@article{kingma_adam:_2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2017-04-19},
	date = {2014-12-22},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Learning}
}

@inproceedings{pevny_discriminative_2016,
	location = {New York, {NY}, {USA}},
	title = {Discriminative Models for Multi-instance Problems with Tree Structure},
	isbn = {978-1-4503-4573-6},
	url = {http://doi.acm.org/10.1145/2996758.2996761},
	doi = {10.1145/2996758.2996761},
	series = {{AISec} '16},
	abstract = {Modelling network traffic is gaining importance to counter modern security threats of ever increasing sophistication. It is though surprisingly difficult and costly to construct reliable classifiers on top of telemetry data due to the variety and complexity of signals that no human can manage to interpret in full. Obtaining training data with sufficiently large and variable body of labels can thus be seen as a prohibitive problem. The goal of this work is to detect infected computers by observing their {HTTP}(S) traffic collected from network sensors, which are typically proxy servers or network firewalls, while relying on only minimal human input in the model training phase. We propose a discriminative model that makes decisions based on a computer's all traffic observed during a predefined time window (5 minutes in our case). The model is trained on traffic samples collected over equally-sized time windows for a large number of computers, where the only labels needed are (human) verdicts about the computer as a whole (presumed infected vs. presumed clean). As part of training, the model itself learns discriminative patterns in traffic targeted to individual servers and constructs the final high-level classifier on top of them. We show the classifier to perform with very high precision, and demonstrate that the learned traffic patterns can be interpreted as Indicators of Compromise. We implement the discriminative model as a neural network with special structure reflecting two stacked multi instance problems. The main advantages of the proposed configuration include not only improved accuracy and ability to learn from gross labels, but also automatic learning of server types (together with their detectors) that are typically visited by infected computers.},
	pages = {83--91},
	booktitle = {Proceedings of the 2016 {ACM} Workshop on Artificial Intelligence and Security},
	publisher = {{ACM}},
	author = {Pevny, Tomas and Somol, Petr},
	urldate = {2017-03-01},
	date = {2016},
	keywords = {big data, learning indicators of compromise, malware detection, neural network, user modeling}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	rights = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	urldate = {2018-04-05},
	date = {1986-10},
	langid = {english}
}

@incollection{nokland_direct_2016,
	title = {Direct Feedback Alignment Provides Learning in Deep Neural Networks},
	url = {http://papers.nips.cc/paper/6441-direct-feedback-alignment-provides-learning-in-deep-neural-networks.pdf},
	pages = {1037--1045},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Nøkland, Arild},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	date = {2016}
}

@article{lillicrap_random_2016,
	title = {Random synaptic feedback weights support error backpropagation for deep learning},
	volume = {7},
	url = {http://dx.doi.org/10.1038/ncomms13276},
	pages = {13276},
	journaltitle = {Nature Communications},
	shortjournal = {Nature Communications},
	author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
	date = {2016-11-08}
}

@incollection{lecun_learning_1986,
	title = {Learning Process in an Asymmetric Threshold Network},
	isbn = {978-3-642-82657-3},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-82657-3_24},
	series = {{NATO} {ASI} Series},
	abstract = {Threshold functions and related operators are widely used as basic elements of adaptive and associative networks [Nakano 72, Amari 72, Hopfield 82]. There exist numerous learning rules for finding a set of weights to achieve a particular correspondence between input-output pairs. But early works in the field have shown that the number of threshold functions (or linearly separable functions) in N binary variables is small compared to the number of all possible boolean mappings in N variables, especially if N is large. This problem is one of the main limitations of most neural networks models where the state is fully specified by the environment during learning: they can only learn linearly separable functions of their inputs. Moreover, a learning procedure which requires the outside world to specify the state of every neuron during the learning session can hardly be considered as a general learning rule because in real-world conditions, only a partial information on the “ideal” network state for each task is available from the environment. It is possible to use a set of so-called “hidden units” [Hinton,Sejnowski,Ackley. 84], without direct interaction with the environment, which can compute intermediate predicates. Unfortunately, the global response depends on the output of a particular hidden unit in a highly non-linear way, moreover the nature of this dependence is influenced by the states of the other cells.},
	pages = {233--240},
	booktitle = {Disordered Systems and Biological Organization},
	publisher = {Springer, Berlin, Heidelberg},
	author = {{LeCun}, Yann},
	urldate = {2018-04-11},
	date = {1986},
	langid = {english},
	doi = {10.1007/978-3-642-82657-3_24}
}

@article{bengio_how_2014,
	title = {How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation},
	url = {http://arxiv.org/abs/1407.7906},
	abstract = {We propose to exploit \{{\textbackslash}em reconstruction\} as a layer-local training signal for deep learning. Reconstructions can be propagated in a form of target propagation playing a role similar to back-propagation but helping to reduce the reliance on derivatives in order to perform credit assignment across many levels of possibly strong non-linearities (which is difficult for back-propagation). A regularized auto-encoder tends produce a reconstruction that is a more likely version of its input, i.e., a small move in the direction of higher likelihood. By generalizing gradients, target propagation may also allow to train deep networks with discrete hidden units. If the auto-encoder takes both a representation of input and target (or of any side information) in input, then its reconstruction of input representation provides a target towards a representation that is more likely, conditioned on all the side information. A deep auto-encoder decoding path generalizes gradient propagation in a learned way that can could thus handle not just infinitesimal changes but larger, discrete changes, hopefully allowing credit assignment through a long chain of non-linear operations. In addition to each layer being a good auto-encoder, the encoder also learns to please the upper layers by transforming the data into a space where it is easier to model by them, flattening manifolds and disentangling factors. The motivations and theoretical justifications for this approach are laid down in this paper, along with conjectures that will have to be verified either mathematically or experimentally, including a hypothesis stating that such auto-encoder mediated target propagation could play in brains the role of credit assignment through many non-linear, noisy and discrete transformations.},
	journaltitle = {{arXiv}:1407.7906 [cs]},
	author = {Bengio, Yoshua},
	urldate = {2018-04-11},
	date = {2014-07-29},
	eprinttype = {arxiv},
	eprint = {1407.7906},
	keywords = {Computer Science - Learning}
}

@article{bengio_towards_2015,
	title = {Towards Biologically Plausible Deep Learning},
	url = {http://arxiv.org/abs/1502.04156},
	abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational {EM} algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
	journaltitle = {{arXiv}:1502.04156 [cs]},
	author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
	urldate = {2018-04-11},
	date = {2015-02-13},
	eprinttype = {arxiv},
	eprint = {1502.04156},
	keywords = {Computer Science - Learning}
}

@online{olah_calculus_2015,
	title = {Calculus on Computational Graphs: Backpropagation},
	url = {https://colah.github.io/posts/2015-08-Backprop/},
	titleaddon = {colah's blog},
	author = {Olah, Christopher},
	urldate = {2018-04-22},
	date = {2015-08-31}
}

@inproceedings{lee_difference_2015,
	title = {Difference Target Propagation},
	isbn = {978-3-319-23528-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-23528-8_31},
	doi = {10.1007/978-3-319-23528-8_31},
	series = {Lecture Notes in Computer Science},
	abstract = {Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of non-linearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks.},
	eventtitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	pages = {498--515},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	publisher = {Springer, Cham},
	author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
	urldate = {2018-05-02},
	date = {2015-09-07},
	langid = {english}
}

@article{bourlard_auto-association_1988,
	title = {Auto-association by multilayer perceptrons and singular value decomposition},
	volume = {59},
	issn = {0340-1200, 1432-0770},
	url = {https://link.springer.com/article/10.1007/BF00332918},
	doi = {10.1007/BF00332918},
	abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Loève transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the rôle of the different parameters.},
	pages = {291--294},
	number = {4},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybern.},
	author = {Bourlard, H. and Kamp, Y.},
	urldate = {2018-05-03},
	date = {1988-09-01},
	langid = {english}
}

@thesis{dedic_hierarchicke_2017,
	location = {Prague},
	title = {Hierarchické modely síťového provozu},
	abstract = {The current approach to the detection of unwanted software by monitoring client traffic uses
handwritten features as part of the model. This approach has several disadvantages. This thesis proposes
a fully automated classifier which recognises malware activity at network connection level. The multi-
instance learning approach was used in order to achieve this. As a part of this thesis the multi-instance
learning was theoretically defined using two different formalisms and current work in this field was sum-
marised. Subsequently, there was described the hierarchical structure of an {URL} address which was used
as an input for the classifier. A model reflecting this inherent hierarchical structure was proposed and
an explanation of how multi-instance learning was utilised and modified was presented together with the
description of the implementation of the model using neural networks. Methods of classifier quality as-
sessment were outlined. The classifier presented here was compared with prior art and the influence of
model parameters on its quality was assessed.},
	pagetotal = {41},
	institution = {Czech Technical University in Prague},
	type = {Bachelor thesis},
	author = {Dědič, Marek},
	date = {2017-07-07}
}

@article{zhang_multi-instance_2009,
	title = {Multi-instance clustering with applications to multi-instance prediction},
	volume = {31},
	issn = {0924-669X, 1573-7497},
	url = {https://www.researchgate.net/profile/Min_Ling_Zhang2/publication/2539374_Neural_Networks_for_Multi-Instance_Learning/links/565d98f508ae4988a7bc857a.pdf},
	doi = {10.1007/s10489-007-0111-x},
	abstract = {In the setting of multi-instance learning, each object is represented by a bag composed of multiple instances instead of by a single instance in a traditional learning setting. Previous works in this area only concern multi-instance prediction problems where each bag is associated with a binary (classification) or real-valued (regression) label. However, unsupervised multi-instance learning where bags are without labels has not been studied. In this paper, the problem of unsupervised multi-instance learning is addressed where a multi-instance clustering algorithm named Bamic is proposed. Briefly, by regarding bags as atomic data items and using some form of distance metric to measure distances between bags, Bamic adapts the popular k -Medoids algorithm to partition the unlabeled training bags into k disjoint groups of bags. Furthermore, based on the clustering results, a novel multi-instance prediction algorithm named Bartmip is developed. Firstly, each bag is re-represented by a k-dimensional feature vector, where the value of the i-th feature is set to be the distance between the bag and the medoid of the i-th group. After that, bags are transformed into feature vectors so that common supervised learners are used to learn from the transformed feature vectors each associated with the original bag’s label. Extensive experiments show that Bamic could effectively discover the underlying structure of the data set and Bartmip works quite well on various kinds of multi-instance prediction problems.},
	pages = {47--68},
	number = {1},
	journaltitle = {Applied Intelligence},
	shortjournal = {Appl Intell},
	author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
	urldate = {2018-05-14},
	date = {2009-08-01},
	langid = {english}
}

@inproceedings{chen_contextual_2012,
	location = {Sichuan, China},
	title = {Contextual Hausdorff dissimilarity for multi-instance clustering},
	isbn = {978-1-4673-0024-7},
	url = {https://ieeexplore.ieee.org/abstract/document/6233889/},
	doi = {10.1109/FSKD.2012.6233889},
	eventtitle = {2012 9th International Conference on Fuzzy Systems and Knowledge Discovery},
	pages = {870--873},
	booktitle = {Fuzzy Systems and Knowledge Discovery ({FSKD})},
	publisher = {{IEEE} Computer Society Press},
	author = {Chen, Ying and Wu, Ou},
	urldate = {2018-05-14},
	date = {2012-05}
}

@inproceedings{pevny_using_2017,
	title = {Using Neural Network Formalism to Solve Multiple-Instance Problems},
	isbn = {978-3-319-59072-1},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-59072-1_17},
	doi = {10.1007/978-3-319-59072-1_17},
	series = {Lecture Notes in Computer Science},
	abstract = {Many objects in the real world are difficult to describe by means of a single numerical vector of a fixed length, whereas describing them by means of a set of vectors is more natural. Therefore, Multiple instance learning ({MIL}) techniques have been constantly gaining in importance throughout the last years. {MIL} formalism assumes that each object (sample) is represented by a set (bag) of feature vectors (instances) of fixed length, where knowledge about objects (e.g., class label) is available on bag level but not necessarily on instance level. Many standard tools including supervised classifiers have been already adapted to {MIL} setting since the problem got formalized in the late nineties. In this work we propose a neural network ({NN}) based formalism that intuitively bridges the gap between {MIL} problem definition and the vast existing knowledge-base of standard models and classifiers. We show that the proposed {NN} formalism is effectively optimizable by a back-propagation algorithm and can reveal unknown patterns inside bags. Comparison to 14 types of classifiers from the prior art on a set of 20 publicly available benchmark datasets confirms the advantages and accuracy of the proposed solution.},
	eventtitle = {International Symposium on Neural Networks},
	pages = {135--142},
	booktitle = {Advances in Neural Networks - {ISNN} 2017},
	publisher = {Springer, Cham},
	author = {Pevný, Tomáš and Somol, Petr},
	urldate = {2018-05-23},
	date = {2017-06-21},
	langid = {english}
}

@article{hahnloser_digital_2000,
	title = {Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit},
	volume = {405},
	rights = {2000 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/35016072},
	doi = {10.1038/35016072},
	abstract = {Digital circuits such as the flip-flop use feedback to achieve multi-stability and nonlinearity to restore signals to logical levels, for example 0 and 1. Analogue feedback circuits are generally designed to operate linearly, so that signals are over a range, and the response is unique. By contrast, the response of cortical circuits to sensory stimulation can be both multistable and graded1,2,3,4. We propose that the neocortex combines digital selection of an active set of neurons with analogue response by dynamically varying the positive feedback inherent in its recurrent connections. Strong positive feedback causes differential instabilities that drive the selection of a set of active neurons under the constraints embedded in the synaptic weights. Once selected, the active neurons generate weaker, stable feedback that provides analogue amplification of the input. Here we present our model of cortical processing as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.},
	pages = {947--951},
	number = {6789},
	journaltitle = {Nature},
	author = {Hahnloser, Richard H. R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
	urldate = {2018-08-27},
	date = {2000-06},
	langid = {english}
}

@article{ramachandran_searching_2018,
	title = {Searching for Activation Functions},
	url = {https://openreview.net/forum?id=SkBYYyZRZ},
	abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the...},
	author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
	urldate = {2018-08-27},
	date = {2018-02-15}
}

@article{liu_limited_1989,
	title = {On the limited memory {BFGS} method for large scale optimization},
	volume = {45},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/BF01589116},
	doi = {10.1007/BF01589116},
	abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-{BFGS} method. We compare its performance with that of the method developed by Buckley and {LeNir} (1985), which combines cycles of {BFGS} steps and conjugate direction steps. Our numerical tests indicate that the L-{BFGS} method is faster than the method of Buckley and {LeNir}, and is better able to use additional storage to accelerate convergence. We show that the L-{BFGS} method can be greatly accelerated by means of a simple scaling. We then compare the L-{BFGS} method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-{BFGS} method. However we find that for other problems the L-{BFGS} method is very competitive due to its low iteration cost. We also study the convergence properties of the L-{BFGS} method, and prove global convergence on uniformly convex problems.},
	pages = {503--528},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Mathematical Programming},
	author = {Liu, Dong C. and Nocedal, Jorge},
	urldate = {2018-08-28},
	date = {1989-08-01},
	langid = {english},
	keywords = {conjugate gradient method, Large scale nonlinear optimization, limited memory methods, partitioned quasi-Newton method}
}

@article{guerguiev_towards_2017,
	title = {Towards deep learning with segregated dendrites},
	volume = {6},
	issn = {2050-084X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5716677/},
	doi = {10.7554/eLife.22901},
	abstract = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi-compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations—the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons., Artificial intelligence has made major progress in recent years thanks to a technique known as deep learning, which works by mimicking the human brain. When computers employ deep learning, they learn by using networks made up of many layers of simulated neurons. Deep learning has opened the door to computers with human – or even super-human – levels of skill in recognizing images, processing speech and controlling vehicles. But many neuroscientists are skeptical about whether the brain itself performs deep learning., The patterns of activity that occur in computer networks during deep learning resemble those seen in human brains. But some features of deep learning seem incompatible with how the brain works. Moreover, neurons in artificial networks are much simpler than our own neurons. For instance, in the region of the brain responsible for thinking and planning, most neurons have complex tree-like shapes. Each cell has ‘roots’ deep inside the brain and ‘branches’ close to the surface. By contrast, simulated neurons have a uniform structure., To find out whether networks made up of more realistic simulated neurons could be used to make deep learning more biologically realistic, Guerguiev et al. designed artificial neurons with two compartments, similar to the ‘roots’ and ‘branches’. The network learned to recognize hand-written digits more easily when it had many layers than when it had only a few. This shows that artificial neurons more like those in the brain can enable deep learning. It even suggests that our own neurons may have evolved their shape to support this process., If confirmed, the link between neuronal shape and deep learning could help us develop better brain-computer interfaces. These allow people to use their brain activity to control devices such as artificial limbs. Despite advances in computing, we are still superior to computers when it comes to learning. Understanding how our own brains show deep learning could thus help us develop better, more human-like artificial intelligence in the future.},
	journaltitle = {{eLife}},
	shortjournal = {{eLife}},
	author = {Guerguiev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
	urldate = {2018-08-31},
	date = {2017},
	pmid = {29205151},
	pmcid = {PMC5716677}
}

@online{crick_recent_1989,
	title = {The recent excitement about neural networks},
	rights = {1989 Nature Publishing Group},
	url = {https://www.nature.com/articles/337129a0},
	abstract = {The recent excitement about neural networks},
	titleaddon = {Nature},
	type = {Comments and Opinion},
	author = {Crick, Francis},
	urldate = {2018-08-31},
	date = {1989-01-12},
	langid = {english},
	doi = {10.1038/337129a0}
}

@article{bartunov_assessing_2018,
	title = {Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures},
	url = {https://openreview.net/forum?id=SyPicjbWQ},
	abstract = {The backpropagation of error algorithm ({BP}) is often said to be impossible to implement in a real brain. The recent success of deep networks in machine learning and {AI}, however, has inspired...},
	author = {Bartunov, Sergey and Santoro, Adam and Richards, Blake A. and Hinton, Geoffrey E. and Lillicrap, Timothy},
	urldate = {2018-09-01},
	date = {2018-06-15}
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	volume = {115},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The {ImageNet} Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	pages = {211--252},
	number = {3},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	urldate = {2018-09-01},
	date = {2015-12-01},
	langid = {english},
	keywords = {Benchmark, Dataset, Large-scale, Object detection, Object recognition}
}

@inproceedings{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
	eventtitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = {249--256},
	booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	author = {Glorot, Xavier and Bengio, Yoshua},
	urldate = {2018-09-01},
	date = {2010-03-31},
	langid = {english}
}

@article{bezanson_julia:_2017,
	title = {Julia: A Fresh Approach to Numerical Computing},
	volume = {59},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/10.1137/141000671},
	doi = {10.1137/141000671},
	shorttitle = {Julia},
	abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be “laws of nature"  by practitioners of numerical computing: {\textbackslash}beginlist {\textbackslash}item  High-level dynamic programs have to be slow. {\textbackslash}item  One must prototype in one language and then rewrite in another language for speed or deployment. {\textbackslash}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. {\textbackslash}endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
	pages = {65--98},
	number = {1},
	journaltitle = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.},
	urldate = {2018-09-01},
	date = {2017-01-01}
}

@article{innes_flux:_2018,
	title = {Flux: Elegant machine learning with Julia},
	doi = {10.21105/joss.00602},
	shorttitle = {Flux},
	journaltitle = {Journal of Open Source Software},
	author = {Innes, Mike},
	date = {2018}
}