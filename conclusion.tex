\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

In this work, the multi-instance paradigm was described using an algebraic formalism. Two methods for credit assignment in learning neural networks were presented - back-propagation and target propagation. The back-propagation algorithm is a well-explored and standard algorithm, whereas target propagation represents a more biologically plausible alternative. Target propagation isn't as well explored, but is backed by experimental results in computational neuroscience (see \cite{guerguiev_towards_2017}). The aim of this work was to apply the target propagation method to multi-instance problems.

A simple experiment was designed to compare the performance of back-propagation and target propagation. The results show that as it was designed and implemented, target propagation is not a viable alternative to back-propagation. While back-propagation based models produce reasonably useful models, the target propagation based models failed to solve the presented problem. \cite{bartunov_assessing_2018} show similar difficulties in using target propagation for harder problems, such as classification on the ImageNet dataset (see \cite{russakovsky_imagenet_2015}).

Due to the inferior performance of the target propagation models, their applicability to multi-instance learning was not further explored. Rather, a deeper inspection of the learning process of the models was concluded, with several hypotheses as to why the model behaves as it does. The poor performance of the model might be attributed to several factors, such as difficulties in learning auto-encoders on low-dimensional data, improper configuration of the model hyperparameters as there are many which could influence the results, or an intrinsic problem with the method itself. The data are not conclusive enough to single out a particular factor, although they point to problems in training the encoder network rather than the decoder network.

Further work in analysing biologically motivated learning algorithms is needed, particularly an evaluation of their mechanics and performance for complex problems such as the aforementioned ImageNet dataset. For target propagation in particular, an analysis of the influence of hyperparameter tuning may be a starting point. Also note that hyperparameter values needn't be constant. Annealing of hyperparameters such as the last layer target parameter \( \eta \) might produce some better results.
